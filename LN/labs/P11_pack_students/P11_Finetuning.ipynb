{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYKTxQbAXTjc"
      },
      "source": [
        "This notebook is due to Margarida Campos (many thanks!!). I (Luísa) included some additional details. Any mistakes are my own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBYewdU_rmyC"
      },
      "source": [
        "#Fine-tuning\n",
        "\n",
        "In this notebook you will use transfer learning to tackle a text classification task.\n",
        "\n",
        "We will fine-tune a BERT-like model on our small dataset in order to take advantage of the knowledge aquired by the pretrained model.\n",
        "\n",
        "**Attention:** Turn on GPU runtime to increase the speed of training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yavGEDg8JBxB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn import model_selection, metrics\n",
        "from datasets import Dataset\n",
        "\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHYfg8i4tmuk"
      },
      "source": [
        "## Getting the Data\n",
        "Let's continue our challenge of classifying spells:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfKrEz4sLzou"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/sample_data/P11_dataset_cast.csv',sep=';')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqCInXf-twkv"
      },
      "source": [
        "Transforming our target (labels) into integers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUgeCtT0JsaE"
      },
      "outputs": [],
      "source": [
        "n_classes = data['category'].unique().shape[0]\n",
        "data[['category']] = data[['category']].astype('category')\n",
        "data['label'] = data['category'].cat.codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUtIwnVJt87E"
      },
      "source": [
        "### Split into train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQUQ2DOWM_M3"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = model_selection.train_test_split(data[['text','label']],\n",
        "                                                     test_size=0.1,\n",
        "                                                     random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVDepYNDuEG6"
      },
      "source": [
        "## Tokenizing\n",
        "\n",
        "We will use [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert) - a lighter model that was trained to mimic BERT's outputs.\n",
        "We will use the [HuggingFace](https://huggingface.co/) library - home to most of the state-of-the-art language models.\n",
        "\n",
        "Different models were trained using different preprocessing steps, so we need to use the appropriate `tokenizer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocMih2zVMCfR"
      },
      "outputs": [],
      "source": [
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name,\n",
        "                                          do_lower_case=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So44UNUYvhit"
      },
      "source": [
        "### What does the tokenizer do?\n",
        "BERT tokenizers, in addition to tokenizing a sentence, add two special tokens:\n",
        " - [CLS] token used for text classification\n",
        " - [SEP] token used for the end of the sentence or separation of sentences when each observation is more than one sentence combined.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MargaridaMCampos/SL_data/main/Screenshot%202025-10-03%20at%2016.45.48.png\" alt=\"drawing\" width=\"300\"/>\n",
        "\n",
        "The outputs are:\n",
        "\n",
        " - `input_ids`: the ids of each token\n",
        " - `attention_mask`: binary vector indicating if words should be considered by the attention mechanism (`0` would appear if the sentences were padded for example: the model should ignore the padded values!)\n",
        "- Check what would happen if you use: tokenizer(\"I love NLP\", padding='max_length', max_length=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdpAIE9rMDjU"
      },
      "outputs": [],
      "source": [
        "sample_text = 'I love NLP'\n",
        "tokens = tokenizer(sample_text, truncation=True)\n",
        "\n",
        "# Test:\n",
        "# tokens = tokenizer(\"I love NLP\", truncation=True, padding='max_length', max_length=10)\n",
        "\n",
        "print(tokens)\n",
        "print(tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])) # to see which were the considered tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES4iQYGXx4ND"
      },
      "source": [
        "We will transform our data into the HuggingFace `Dataset` format which will make our lifes easier, specially with larger datasets!\n",
        "With this, we will be converting our raw data (like text files, CSVs, JSON, etc.) into the standardized Dataset object provided by the Hugging Face datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxuLaCPcKmLF"
      },
      "outputs": [],
      "source": [
        "# Convert data into Dataset structure\n",
        "train_dataset = Dataset.from_pandas(df_train)\n",
        "test_dataset = Dataset.from_pandas(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF2RJqqAyXsN"
      },
      "source": [
        "Let's tokenize our data!\n",
        "If you want to add preprocessing steps you can do so inside the function and then we map it to our `Dataset` structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ttZ4KHYK9PA"
      },
      "outputs": [],
      "source": [
        "def tokenize_data(dataset):\n",
        "    return tokenizer(dataset[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_data, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_data, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0g_7DrGymEo"
      },
      "source": [
        "## Training\n",
        "\n",
        "Let's load our pretrained model! We can make use of HuggingFace's pre-built architectures: using `AutoModelForSequenceClassification` we are already importing the BERT architecture adapted for text classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xiibs_CgOTQO"
      },
      "outputs": [],
      "source": [
        "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name,\n",
        "                                                                        num_labels = n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKS6pOLfzDwY"
      },
      "source": [
        "### Metrics\n",
        "\n",
        "BERT outputs *logits* - a vector of scores for each class.\n",
        "Let's define some evaluation functions to monitor the progress of our model!\n",
        "\n",
        "*Note:* Uncomment the below cell if you're in Colab or you don't have the `evaluate` package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu79WLC4QRez"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F_zm2AuQKXx"
      },
      "outputs": [],
      "source": [
        "# load metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1) # obtaining the class with the highest score\n",
        "    return {\n",
        "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws0wwXS-0edL"
      },
      "source": [
        "Let us call a padding operator to give the model. You can choose and pass additional padding parameters to it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_CnN7cD0d_4"
      },
      "outputs": [],
      "source": [
        "# Prepare data collator for padding sequences\n",
        "data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Note: it chooses the max sequence length in the batch, not a fixed global max length efficient!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0uQJNlJw6x_"
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "\n",
        "samples = [\n",
        "    tokenizer(\"I love NLP\"),\n",
        "    tokenizer(\"I love models as Transformers\")\n",
        "]\n",
        "\n",
        "batch = data_collator(samples)\n",
        "print(batch.keys()) # notice the 0 at the end of the input_ids (try to explain it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMYNFpkb0tjL"
      },
      "source": [
        "### Defining training parameters\n",
        "\n",
        "In HuggingFace we define our training hyperparameters in `TrainingArguments`.\n",
        "Please, spend some time here if these concepts are unfamiliar to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYHpVNjtPjyO"
      },
      "outputs": [],
      "source": [
        "# Defining the training arguments\n",
        "\n",
        "# Remember:\n",
        "# a) Epoch: one full pass through the entire training dataset.\n",
        "# b) Step (or iteration):\tone batch of data is passed through the model (forward + backward pass).\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir=\"results\", # where to save the resulting trained model\n",
        "    learning_rate=2e-5, # learning rate\n",
        "    per_device_train_batch_size=16, # batch size for training\n",
        "    per_device_eval_batch_size=16, # batch size for evaluation (don't need to do a full evaluation)\n",
        "    num_train_epochs=5, # epochs (\"epoch\" is also used internally to track which epoch you’re on)\n",
        "    weight_decay=0.01, # penalizing very large weights\n",
        "    eval_strategy=\"epoch\", # evaluating at each epoch (it could be every N steps)\n",
        "    logging_strategy=\"steps\", # when to log information (like training loss, learning rate, etc.).\n",
        "    logging_steps=50, # After every 50 batches (steps), record training metrics\n",
        "    seed=42,      #setting seeds -- global random seed (for reproducibility) -- ensures model weight initialization is the same\n",
        "    data_seed=42, # ensures the dataset split (e.g., 80/20 train/val) is the same.\n",
        "    report_to=\"none\") # avoid API requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rRdQqRG2I0g"
      },
      "source": [
        "### Defining our training process\n",
        "\n",
        "We define our finetuning in the `Trainer` object: what is the model, what is the data, the tokenizer, the padder, the metrics to use..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "695F_VMw2Jhi"
      },
      "outputs": [],
      "source": [
        "# Split: 90% train, 10% validation\n",
        "split = tokenized_train.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "tokenized_train = split['train']       # 90% for training\n",
        "tokenized_validation = split['test']\n",
        "\n",
        "# Define Trainer object for training the model\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_validation, # not tokenized_test,\n",
        "    processing_class=tokenizer, # the tokenizer will handle preprocessing of text data\n",
        "    data_collator=data_collator, # process and batch examples dynamically before feeding them to the model (pads, ...)\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDvHppVA2k79"
      },
      "source": [
        "## Let's finetune!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDWVW2KG063U"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save_model('model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oNN6SV02pZK"
      },
      "source": [
        "### Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU9yD0EQLnPW"
      },
      "outputs": [],
      "source": [
        "eval_metrics = trainer.evaluate()\n",
        "print(\"Eval metrics:\", eval_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of7Ehray2x7K"
      },
      "source": [
        "## Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16Xw616rPfo3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Run inference on the tokenized test set\n",
        "predictions = trainer.predict(tokenized_test)\n",
        "\n",
        "# Get the raw logits\n",
        "logits = predictions.predictions\n",
        "\n",
        "# Convert to class IDs\n",
        "y_pred = np.argmax(logits, axis=-1)\n",
        "\n",
        "# True labels are here:\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(metrics.classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Biid3BiB22jE"
      },
      "source": [
        "## Let's try to make it better!\n",
        "\n",
        " - Increase the number of epochs to `10`. Is the model still learning?\n",
        " - Do a little research on lighter language models for classification and experiment with a different one (always checking the HuggingFace documentation!)\n",
        "\n",
        " We are using the tokenizer and the model with the default arguments. Check the [documentation](https://huggingface.co/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertConfig) and play with some parameters of your choice - think about what they mean!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88d6bprP3vUP"
      },
      "source": [
        "## Freezing parameters\n",
        "\n",
        "Sometimes we do not have the compute resources to finetune the large number of parameters of the pretrained models. One common approach is to *freeze* the language model's weights - use them as they are - and train **only** the weights of the last layers - the classifier part: using the pretrained model essentially as a *feature provider*!\n",
        "\n",
        "Add the following code after the loading of the model (that is, after model = AutoModelForSequenceClassification.from_pretrained...)\n",
        "\n",
        "Notice that:\n",
        "- All parameters — both in model.distilbert and in model.classifier — start with requires_grad = True\n",
        "\n",
        "# Freeze all layers except the classifier (“freezes” the DistilBERT encoder)\n",
        "for param in model.distilbert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# If you want to make explicit that only the classification head is trainable\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# If you want to unfreeze the last two layers\n",
        "# 1) Freeze everything first\n",
        "for param in model.distilbert.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2) Unfreeze the last two transformer layers\n",
        "for layer in model.distilbert.transformer.layer[-2:]:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3KrayLITzza"
      },
      "outputs": [],
      "source": [
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}