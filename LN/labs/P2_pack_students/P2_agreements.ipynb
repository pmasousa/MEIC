{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "| Kappa value | Strength of agreement | Notes                                                    |\n",
        "| ----------- | --------------------- | -------------------------------------------------------- |\n",
        "| ≤ 0.00      | Poor                  | Agreement is no better than chance, or worse than chance |\n",
        "| 0.01 – 0.20 | Slight                | Very low agreement beyond chance                         |\n",
        "| 0.21 – 0.40 | Fair                  | Some agreement, but not strong                           |\n",
        "| 0.41 – 0.60 | Moderate              | Reasonable consistency between raters                    |\n",
        "| 0.61 – 0.80 | Substantial           | Good agreement between raters                            |\n",
        "| 0.81 – 1.00 | Almost perfect        | Very high agreement between raters                       |\n"
      ],
      "metadata": {
        "id": "6QCPJlgqbbGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaUhMUBhUrWF",
        "outputId": "2247fdd9-b952-4d17-9967-9c3972363719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kappa:  0.1666666666666666\n",
            "p-value (one_sided):  0.35469405750711325\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats import inter_rater\n",
        "\n",
        "# for each criterion\n",
        "rater_1 = [1, 0, 1, 1, 0]\n",
        "rater_2 = [1, 1, 0, 1, 0]\n",
        "\n",
        "# Cosntroi a contigency table (cross-tabulation of counts)\n",
        "# Row = rating by rater 1\n",
        "# Column = rating by rater 2\n",
        "# Cell = count of times that combination occurred\n",
        "# | rater\\_2 | 0 | 1 |\n",
        "# | -------- | - | - |\n",
        "# | rater\\_1 |   |   |\n",
        "# | **0**    | 1 | 1 |\n",
        "# | **1**    | 1 | 2 |\n",
        "\n",
        "contingency = pd.crosstab(rater_1, rater_2, margins = False) # margins = False don't add row/column totals.\n",
        "\n",
        "res = inter_rater.cohens_kappa(contingency)\n",
        "\n",
        "# Kappa value\n",
        "print(\"kappa: \", res.kappa)\n",
        "\n",
        "# p-value tests whether the observed agreement is significantly greater than chance\n",
        "# pvalue_one_sided if you only care about agreement being better than chance\n",
        "print(\"p-value (one_sided): \", res. pvalue_one_sided)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaining results:\n",
        "- Null hypothesis (H₀): The agreement between raters is no better than chance (remember: kappa = 0, no agreement (only chance-level)).\n",
        "\n",
        "- If p-value = 0.355, 0.355 > 0.05 (assuming a common significance level α = 0.05), means you fail to reject H₀.\n",
        "\n",
        "Thus:\n",
        "- the observed agreement could plausibly be due to random chance. You don't have strong statistical evidence that the raters agree beyond chance."
      ],
      "metadata": {
        "id": "lKjFqfi3YV_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Fleiss’ kappa | Strength of agreement |\n",
        "| ------------- | --------------------- |\n",
        "| ≤ 0.00        | Poor (no agreement)   |\n",
        "| 0.01 – 0.20   | Slight                |\n",
        "| 0.21 – 0.40   | Fair                  |\n",
        "| 0.41 – 0.60   | Moderate              |\n",
        "| 0.61 – 0.80   | Substantial           |\n",
        "| 0.81 – 1.00   | Almost perfect        |\n"
      ],
      "metadata": {
        "id": "j9XWbSSHbUDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example ratings from 3 raters on 5 items (rows = items, cols = raters)\n",
        "# Each row corresponds to a subject/item, each column to a rater’s decision (0 = no, 1 = yes)\n",
        "ratings = [\n",
        "    [1, 0, 1],  # Item 1\n",
        "    [1, 1, 1],  # Item 2\n",
        "    [0, 0, 0],  # Item 3\n",
        "    [1, 1, 0],  # Item 4\n",
        "    [0, 1, 0],  # Item 5\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(ratings, columns=[\"rater_1\", \"rater_2\", \"rater_3\"])\n",
        "\n",
        "# Convert ratings into the required format: count of raters per category for each item\n",
        "# e.g., for each row: how many raters said 0, how many said 1\n",
        "table = df.apply(pd.Series.value_counts, axis=1).fillna(0).astype(int)\n",
        "\n",
        "# Fleiss' kappa\n",
        "# Notice that Fleiss’ kappa (multi-rater agreement) is implemented as a measure only in\n",
        "# statsmodels, not as a full hypothesis test — so you just get the statistic\n",
        "fleiss = inter_rater.fleiss_kappa(table.values)\n",
        "\n",
        "print(f\"Fleiss’ kappa = {fleiss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX1jemNXWM5l",
        "outputId": "026eef71-6b39-4b60-b2f2-b2d1443ac87b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fleiss’ kappa = 0.196\n"
          ]
        }
      ]
    }
  ]
}