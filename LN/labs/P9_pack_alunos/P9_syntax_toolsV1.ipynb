{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyZx55TWTbyg"
      },
      "source": [
        "# Testing Natural Language Processing libraries: SpaCy and stanza\n",
        "\n",
        "Some of the following examples and code snippets were taken/adapted from:\n",
        "\n",
        "(a) spaCy webpage: https://spacy.io\n",
        "\n",
        "(b) stanza github: https://stanfordnlp.github.io/stanza/installation_usage.html\n",
        "\n",
        "(c) a notebook from Fernando Batista and Ricardo Ribeiro, my colleagues and dear friends from ISCTE. Thanks! (any mistake is on me)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlZ5SbGJoezz"
      },
      "source": [
        "**1. Install** (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peN8Hl1rUMCz"
      },
      "outputs": [],
      "source": [
        "# !pip install -U pip setuptools wheel\n",
        "# !pip install -U spacy\n",
        "!pip install -U stanza # This is problebly needed\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8v1iQM74U0Dl"
      },
      "source": [
        "**2. Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXI8Sgc7oibw"
      },
      "outputs": [],
      "source": [
        "# Spacy <-- widely used in NLP\n",
        "import spacy\n",
        "from spacy import displacy  # for visualization (see below)\n",
        "\n",
        "# Stanza <-- widely used in NLP\n",
        "import stanza"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zomt-HMlVQGJ"
      },
      "source": [
        "**3. Load**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cOG6ci7VSc3"
      },
      "outputs": [],
      "source": [
        "# Spacy\n",
        "nlp_spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Stanza\n",
        "stanza.download('en')\n",
        "nlp_stanza_en = stanza.Pipeline('en')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ejp3OI7fj_NG"
      },
      "source": [
        "**4. Text to process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miWcDlZ6ZJNu"
      },
      "outputs": [],
      "source": [
        "# We will test with a simple text in English and an external text in Portuguese\n",
        "\n",
        "# Simple text in English\n",
        "my_text_en = \"\"\"Natural Language is my favourite course ever. I just love it.\"\"\"\n",
        "print(\"EN: \", my_text_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGkkH-r5Y-nF"
      },
      "source": [
        "**4.1 Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "701ZECSyTbyo"
      },
      "outputs": [],
      "source": [
        "# Spacy\n",
        "my_text_en_spacy = my_text_en.replace(\"\\n\", \" \") # you may need this for other texts\n",
        "doc_spacy_en = nlp_spacy_en(my_text_en_spacy)\n",
        "\n",
        "print(doc_spacy_en[:10]) # print first tokens\n",
        "print(\" + \".join([token.text for token in doc_spacy_en])) # show tokens split by +"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stanza\n",
        "doc_stanza_en = nlp_stanza_en(my_text_en)\n",
        "\n",
        "# access and print the first Ntokens\n",
        "def my_print(doc):\n",
        "  tokens = []\n",
        "  for sentence in doc.sentences:\n",
        "     for token in sentence.tokens:\n",
        "        tokens.append(token.text)\n",
        "  return tokens\n",
        "\n",
        "print(my_print(doc_stanza_en))"
      ],
      "metadata": {
        "id": "Pox3DDGSjTaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3gbBXPHTbyo"
      },
      "source": [
        "**4.2. Multi-generator (POS, lemma. etc.)**\n",
        "\n",
        "a) spaCy\n",
        "\n",
        "*Text*: The original word text.<br>\n",
        "*Lemma*: The base form of the word.<br>\n",
        "*POS*: The simple UPOS part-of-speech tag.<br>\n",
        "*Tag*: The detailed part-of-speech tag.<br>\n",
        "*Dep*: Syntactic dependency, i.e. the relation between tokens.<br>\n",
        "*Shape*: The word shape â€“ capitalization, punctuation, digits.<br>\n",
        "*is alpha*: Is the token an alpha character?<br>\n",
        "*is stop*: Is the token part of a stop list, i.e. the most common words of the language?\n",
        "\n",
        "b) Stanza\n",
        "\n",
        "*PoS*\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YJ8DemSTbyp"
      },
      "outputs": [],
      "source": [
        "# spaCy\n",
        "for token in doc_spacy_en:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.is_alpha, token.is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stanza\n",
        "nlp_pos_en = stanza.Pipeline(lang='en', processors='tokenize,pos', tokenize_pretokenized=True)\n",
        "doc_stanza_en = nlp_pos_en(my_text_en)\n",
        "doc_en = nlp_pos_en(doc_stanza_en)\n",
        "print(\"{:C}\".format(doc_en))"
      ],
      "metadata": {
        "id": "3jyGzHZ8kLR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeAuk0__qB1v"
      },
      "source": [
        "**4.3. Syntactic Parsing**\n",
        "\n",
        "- dependencies with spaCy\n",
        "- constituents with stanza\n",
        "- vizualization with spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbAYw3qbp5Ih"
      },
      "outputs": [],
      "source": [
        "# spaCy: dependency\n",
        "# Try the different options\n",
        "options = {\"compact\": True, \"bg\": \"#09a3d5\", \"color\": \"white\", \"font\": \"Source Sans Pro\"} # Try!\n",
        "# options = {}\n",
        "\n",
        "# displacy.render(doc_spacy_en[:10], style=\"dep\", options=options) # server instead of render if not in a notebook\n",
        "html = displacy.render(list(doc_spacy_en.sents)[:1], style=\"dep\", options=options, jupyter=False)\n",
        "\n",
        "# check the syntactic dependencies in the generated deps.html (under Files or local folder)\n",
        "with open(\"deps.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "  f.write(html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stanza: constituency\n",
        "nlp_stanza_en = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency')\n",
        "\n",
        "doc_stanza_en = nlp_stanza_en(my_text_en)\n",
        "for sentence in doc_stanza_en.sentences:\n",
        "    print(sentence.constituency)"
      ],
      "metadata": {
        "id": "l-oHs6v3krZ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11",
      "language": "python",
      "name": "py311"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}