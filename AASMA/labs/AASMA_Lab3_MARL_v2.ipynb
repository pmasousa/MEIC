{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Learning in Multiagent Systems\n",
        "\n",
        "In this lab we will study different learning approaches in the context of multiagent systems. We will start by implementing learning methods in the context of normal form games. Then we will study different algorithms in the context of Markov games.\n",
        "\n",
        "Best reference for this topic is chapter 6 of https://www.marl-book.com/\n",
        "\n"
      ],
      "metadata": {
        "id": "Klu6TkY_YG7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n",
        "\n",
        "For this lab we will only need numpy. The toolbox nashpy has already several algorithms implemented and allows to verify some of the results.\n",
        "\n",
        "## nashpy toolbox\n",
        "Knight, V., & Campbell, J. (2018). Nashpy: A Python library for the computation of Nash equilibria. Journal of Open Source Software, 3(30), 904.\n",
        "https://nashpy.readthedocs.io/en/stable/text-book/index.html\n",
        "https://nashpy.readthedocs.io/en/stable/\n"
      ],
      "metadata": {
        "id": "MW0ctoPZaET9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nashpy\n",
        "import nashpy as nash\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "clear_output(wait=False)"
      ],
      "metadata": {
        "id": "qh9Xqy9saG3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repeated Games\n",
        "\n",
        "We will start by defining several standard normal form games in Nashpy toolbox.\n",
        "This toolbox already implements several algorithms to find Nash Equilibria and we can use it to verify the solutions for later examples."
      ],
      "metadata": {
        "id": "VxPw4youL5my"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI6nIOytezx4"
      },
      "outputs": [],
      "source": [
        "# @title Define Games\n",
        "\n",
        "import warnings\n",
        "\n",
        "Games = {}\n",
        "A = np.array([[1, -1], [-1, 1]])\n",
        "Games['Matching pennis'] = nash.Game(A)\n",
        "\n",
        "A = np.array([[-1, -9], [0,-6]])\n",
        "B = np.array([[-1, 0], [-9,-6]])\n",
        "Games['Prisioner dilema'] = nash.Game(A,B)\n",
        "\n",
        "A = np.array([[-2, 6], [0, 3]])\n",
        "B = np.array([[-2, 0], [6, 3]])\n",
        "Games['Hawk Dove'] = nash.Game(A,B)\n",
        "\n",
        "A = np.array([[0,-1,1],[1,0,-1],[-1,1,0]])\n",
        "Games['Rock Paper Scissor'] = nash.Game(A)\n",
        "\n",
        "A = np.array([[10,1],[8,5]])\n",
        "B = np.array([[10,8],[1,5]])\n",
        "Games['Stag Hunt'] = nash.Game(A,B)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Auxiliary functions\n",
        "\n",
        "# get value of a play in a nashpy game\n",
        "def get_value(g,actA,actB):\n",
        "    return g.payoff_matrices[0][actA,actB],g.payoff_matrices[1][actA,actB]\n",
        "\n",
        "# egreedy function\n",
        "# e is the probability of choosing the best action\n",
        "def egreedy(v,e=0.95):\n",
        "    NA = len(v)\n",
        "    b = np.isclose(v,np.max(v))\n",
        "    no = np.sum(b)\n",
        "    if no<NA:\n",
        "        p = b*e/no+(1-b)*(1-e)/(NA-no)\n",
        "    else:\n",
        "        p = b/no\n",
        "\n",
        "    return int(np.random.choice(np.arange(NA),p=p))\n",
        "\n",
        "\n",
        "def plothistofplay(Logn):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(projection='3d')\n",
        "    hist, xedges, yedges = np.histogram2d(Logn[:,0], Logn[:,1], bins=(np.max(Logn[:,0:2])+1), range=[[-0.5, np.max(Logn[:,0:2])+0.5], [-0.5, np.max(Logn[:,0:2])+0.5]])\n",
        "\n",
        "    # Construct arrays for the anchor positions of the 16 bars.\n",
        "    xpos, ypos = np.meshgrid(xedges[:-1] + 0.25, yedges[:-1] + 0.25, indexing=\"ij\")\n",
        "    xpos = xpos.ravel()\n",
        "    ypos = ypos.ravel()\n",
        "    zpos = 0\n",
        "\n",
        "    # Construct arrays with the dimensions for the 16 bars.\n",
        "    dx = dy = 0.5 * np.ones_like(zpos)\n",
        "    dz = hist.ravel()\n",
        "\n",
        "    ax.bar3d(xpos, ypos, zpos, dx, dy, dz, zsort='average')\n",
        "    ax.set_xlabel('row player', fontsize=12, rotation=0)\n",
        "    ax.set_ylabel('column player', fontsize=12, rotation=150)\n",
        "    ax.set_xticks(np.arange(1+np.max(Logn[:,0:2])))\n",
        "    ax.set_yticks(np.arange(1+np.max(Logn[:,0:2])))\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "IeaoRmFldY1c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Game solutions based on Nashpy\n",
        "\n",
        "for kk in Games.keys():\n",
        "    g = Games[kk]\n",
        "\n",
        "    if g.zero_sum:\n",
        "        a = g.linear_program()\n",
        "    else:\n",
        "        a = g.vertex_enumeration()\n",
        "        #b = g.support_enumeration()\n",
        "    print(kk,\"\\n Nash equilibria\\n\",list(a))\n",
        "\n",
        "    # Ficticious Play\n",
        "    np.random.seed(0)\n",
        "    iterations = 5000\n",
        "    play_counts = g.fictitious_play(iterations=iterations)\n",
        "    play_counts=np.array(list(play_counts))\n",
        "    print(\" ficticious play\\n\", play_counts[-1,:,:]/(play_counts.shape[0]-1))"
      ],
      "metadata": {
        "id": "m7BjnLJ6uDvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Useful links:\n",
        "*   https://nashpy.readthedocs.io/en/stable/text-book/normal-form-games.html\n",
        "*   https://nashpy.readthedocs.io/en/stable/text-book/zero-sum-games.html\n",
        "*   https://nashpy.readthedocs.io/en/stable/text-book/vertex-enumeration.html\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mlhNY1inyZPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repeated Games\n",
        "\n",
        "Consider the previous normal form games as a repeated game, we will implement 2 different approaches:\n",
        "* independent learning (refer to Sec. 5.3.2 https://www.marl-book.com/);\n",
        "* ficticious play (refer to Sec. 6.3.1 https://www.marl-book.com/);\n",
        "\n",
        "In most cases you can use the Q-Learning approach to compute the value of a given action:\n",
        "\n",
        "$Q(s_t,a_t) = Q(s_t,a_t) + \\alpha (r + \\gamma \\max_b Q(s_{t+1},b) - Q(s_t,a_t))$\n",
        "\n",
        "in the case of normal form games we do not have state and so we can use:\n",
        "\n",
        "$Q(a_t) = Q(a_t) + \\alpha (r - Q(a_t))$\n",
        "\n",
        "## Independent Learning\n",
        "In independent learning the agents are unaware of other agents in the environment.\n",
        "\n",
        "In this case what information about the environment are they using?\n",
        "\n",
        "## Ficticious Play\n",
        "In ficticious play the agent learn a model of the behaviour of the other agents and then play the best response to that model.\n",
        "In this case what information about the environment are they using?\n",
        "How can they learn the model of the behavior?\n"
      ],
      "metadata": {
        "id": "fqHQeyOscIys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Independent Learning\n",
        "# Independent learners are unaware of the oponents, in practice the oponnents are part of the environment.\n",
        "\n",
        "for kk in Games.keys():#['Prisioner dilema']:#\n",
        "    # Select a game.\n",
        "    g = Games[kk]\n",
        "    # Get the number of actions for each player.\n",
        "    n0, n1 = g.payoff_matrices[0].shape\n",
        "\n",
        "    # Start by defining a Q function for each agent. What should be its size?\n",
        "    Q=[[],[]]\n",
        "    # TODO\n",
        "    # Q[0] =\n",
        "    # Q[1] =\n",
        "\n",
        "    # Lets create a log to track the learning progress and check if an equilibria is reached.\n",
        "    Log = []\n",
        "    A = [-1,-1]\n",
        "    for ii in range(5000):\n",
        "        # Select action for each agent (use an $\\epsilon-greedy$ stretegy).\n",
        "        # TODO\n",
        "        # A[0] =\n",
        "        # A[1] =\n",
        "        r = get_value(g,A[0],A[1])\n",
        "        Log.append([A[0],A[1],r[0],r[1]])\n",
        "\n",
        "        # Update the estimate of the value Q-function for each agent (you can use a learning rate of 0.1).\n",
        "        for aa in range(2):\n",
        "            # TODO\n",
        "            # Q[aa] =\n",
        "\n",
        "    if g.zero_sum:\n",
        "        a = g.linear_program()\n",
        "    else:\n",
        "        a = g.vertex_enumeration()\n",
        "\n",
        "    print(kk,\"\\n Nash equilibria\\n\",list(a))\n",
        "    print(\"Q\\n\",Q[0],\"\\n\",Q[1])\n",
        "    plothistofplay(np.array(Log))\n"
      ],
      "metadata": {
        "id": "2DfiVHRrbFn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Ficticious play\n",
        "# In Ficticious play the learners are aware of the oponents.\n",
        "\n",
        "def best_response(A,B):\n",
        "    a = np.argmax(A)\n",
        "    b = np.argmax(B)\n",
        "    return a,b\n",
        "\n",
        "for kk in Games.keys():\n",
        "    # Select a game.\n",
        "    g = Games[kk]\n",
        "\n",
        "    # Get the number of actions for each player.\n",
        "    n0, n1 = g.payoff_matrices[0].shape\n",
        "\n",
        "    C = np.zeros((n0,n1))+1e-6\n",
        "\n",
        "    # Lets create a log to track the learning progress and check if an equilibria is reached.\n",
        "    Log = []\n",
        "    A = [-1,-1]\n",
        "    for ii in range(5000):\n",
        "\n",
        "        # Select action for each agent.\n",
        "        # TODO\n",
        "        # policyA =\n",
        "        # policyB =\n",
        "        # A[0] =\n",
        "        # A[1] =\n",
        "\n",
        "        C[A[0],A[1]] += 1\n",
        "\n",
        "        r = get_value(g,A[0],A[1])\n",
        "        Log.append([A[0],A[1],r[0],r[1]])\n",
        "\n",
        "    if g.zero_sum:\n",
        "        a = g.linear_program()\n",
        "    else:\n",
        "        a = g.vertex_enumeration()\n",
        "\n",
        "    print(kk,\"\\n Nash equilibria\\n\",list(a))\n",
        "    plothistofplay(np.array(Log))"
      ],
      "metadata": {
        "id": "foZrKtpQAYG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question\n",
        "\n",
        "* Compare the behavior of ficticious play and independent learning.\n",
        "* Can we find Nash equilibria in games using learning algorithms?\n",
        "* When does Q-learning converge to Nash equilibria?\n",
        "* What does ficticious play assume? Can we do even better?"
      ],
      "metadata": {
        "id": "pyIQIwXELPmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Games\n",
        "\n",
        "In this section we will expand the learning approaches seen in repeated games to Markov Games, i.e. games that have a state that changes with the actions made by the agents. In the general, the transitions can be stochastic, but for simplicity we will assume deterministic transitions.\n",
        "\n",
        "We will consider a Stag-Hunt game where two agents can either capture a Stag or a Hare. In this simplified problem the environment is just a corridor with 6 cells. The actions are 0,1,2 corresponding to moving (left, stop, right). The rules of the game are as follows:\n",
        "* the reward is 0 at any step except if either the stag or the hare are captured\n",
        "* the hare can be captured by any agent individually (the agent that captures receives .8 the other .1)\n",
        "* the hare can be captured by the two agents simultaneously (both receive .5)\n",
        "* the stag needs the two agents to capture it simultaneously (both receive 1.)\n",
        "* if either agent occupies the same cell as the hare or the stag they make noise and the preys escape ending the episode\n",
        "\n",
        "The state is represented as a vector of dimension four as follows [agent0-position, agent1-position, stag-position, hare-position]. The following situation would correspond to the state x=[3,3,0,5]\n",
        "\n",
        " \\begin{pmatrix}\n",
        "  |S|& |& |&A_0, A_1 |& |&H|\n",
        " \\end{pmatrix}\n",
        "\n",
        " step(x,[0,2]) would result in state x = [2,4,0,5]\n",
        " \\begin{pmatrix}\n",
        "  |S|& |&A_0 |& |&A_1 |&H|\n",
        " \\end{pmatrix}\n"
      ],
      "metadata": {
        "id": "7AEnjnOYL0iI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "#https://en.wikipedia.org/wiki/Stag_hunt\n",
        "\n",
        "#Stag hunt environment\n",
        "# we consider it as a corridor with NL cells\n",
        "NL = 6\n",
        "# state is a vector of dimension four as follows [agent0-position, agent1-position, stag-position, hare-position]\n",
        "# the actions are 0,1,2 corresponding to moving left, stop, right\n",
        "# the reward is 0 at any step except if either the stag or the hare are captured\n",
        "# the hare can be captured by any agent individually\n",
        "# the stag needs the two agents to capture it simultaneously\n",
        "# if either agent occupies the same cell as the hare or the stag they make noise and the preys escape ending the episode\n",
        "\n",
        "\n",
        "# the function step receives a state and a vector of actions and returns the new state and the reward for each agent\n",
        "# if the location of either the stag of the hare is -1 is means they escapted and the episode should finish\n",
        "def step(x,A):\n",
        "    nx = x[:] # state is [agent0 position, agent1 position, stag position, hare position]\n",
        "    nx[0] = int(np.clip(x[0] + (A[0]-1),0, NL-1))\n",
        "    nx[1] = int(np.clip(x[1] + (A[1]-1),0, NL-1))\n",
        "    r = [0.,0.]\n",
        "    if (nx[0]==nx[3]) & (nx[1]==nx[3]):\n",
        "        r[0] = .5\n",
        "        r[1] = .5\n",
        "        nx[3] = -1\n",
        "        nx[2] = -1\n",
        "    elif (nx[0]==nx[2]) & (nx[1]==nx[2]):\n",
        "        r[0] = 1\n",
        "        r[1] = 1\n",
        "        nx[3] = -1\n",
        "        nx[2] = -1\n",
        "    elif (nx[0]==nx[2]) | (nx[1]==nx[2]):\n",
        "        r[0] = 0\n",
        "        r[1] = 0\n",
        "        nx[3] = -1\n",
        "        nx[2] = -1\n",
        "    elif (nx[0]==nx[3]) & (nx[1]!=nx[3]):\n",
        "        r[0] = .8\n",
        "        r[1] = .1\n",
        "        nx[3] = -1\n",
        "        nx[2] = -1\n",
        "    elif (nx[0]!=nx[3]) & (nx[1]==nx[3]):\n",
        "        r[0] = .1\n",
        "        r[1] = .8\n",
        "        nx[3] = -1\n",
        "        nx[2] = -1\n",
        "    else:\n",
        "        r = [0, 0]\n",
        "\n",
        "    return nx, r"
      ],
      "metadata": {
        "id": "mh737npC5Sn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "\n",
        "* Assuming that the agents start in cell 3 what is the optimal policy for different values of $\\gamma$?\n",
        "Compute a normal-form game considering only the actions going left and going right (until the episode ends).\n",
        "\n",
        "* Assuming that the agent 0 starts in cell 1, for $\\gamma=0.9$ on which cell does agent 1 needs to start to prefer going left or going right?\n",
        "\n",
        "* Assuming that the episode does not end after capturing the hare. Discuss what might happen."
      ],
      "metadata": {
        "id": "lTJit9c1VA1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_X6ElaV0fzRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class definition for the agents\n",
        "\n",
        "This class implements:\n",
        "* independent learners with partial state (unaware of others, refer to Sec. 5.3.2 https://www.marl-book.com/);\n",
        "* independent learners with full state (aware of others);\n",
        "* centralized learning (assuming the sum of rewards, refer to Sec. 5.3.1 https://www.marl-book.com/);\n",
        "* joint-action learners with agent modelling (an extension of ficticious play for markov games, refer to Sec. 6.3.2 https://www.marl-book.com/);\n",
        "\n",
        "\n",
        "Questions:\n",
        "* What is the state for each type of agent?\n",
        "* What and how are they learning about the environment and agents?\n",
        "* How do they select the actions to make?\n"
      ],
      "metadata": {
        "id": "gsvicFoXN24z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flK8QELOOvPx"
      },
      "outputs": [],
      "source": [
        "# @title Class definition for the agents\n",
        "class Agent:\n",
        "    def __init__(self, NS, NA, id, NL = NL, alpha = 0.1, gamma = 0.9, agentType = 'independent'):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.id = id\n",
        "        self.agentType = agentType\n",
        "        self.NL = NL\n",
        "        self.NA = NA\n",
        "\n",
        "        if self.agentType == 'independent':\n",
        "            self.Q = np.ones((NL,NA))*2\n",
        "        elif self.agentType == 'observ':\n",
        "            self.Q = np.ones((NL*NL,NA))*2\n",
        "        elif self.agentType == 'central':\n",
        "            self.Q = np.ones((NL*NL,NA*NA))*2\n",
        "        elif self.agentType == 'JALAM':\n",
        "            self.Q = np.ones((NL*NL,NA,NA))*2\n",
        "            self.C = np.zeros((NL*NL,NA,NA))+0.001\n",
        "\n",
        "    def __str__(self):\n",
        "\n",
        "        return f\"#{self.id} Qshape{self.Q.shape}\"\n",
        "\n",
        "    # this function takes the whole environment state and projects it in the state that each type of agent has access\n",
        "    def observ2state(self, x):\n",
        "        if self.agentType == 'independent':\n",
        "            return x[self.id]\n",
        "        elif self.agentType == 'observ':\n",
        "            return np.ravel_multi_index(x[0:2],[self.NL,self.NL])\n",
        "        elif self.agentType == 'central':\n",
        "            return np.ravel_multi_index(x[0:2],[self.NL,self.NL])\n",
        "        elif self.agentType == 'JALAM':\n",
        "            return np.ravel_multi_index(x[0:2],[self.NL,self.NL])\n",
        "\n",
        "    # this function is the learning update after any iteration with the environment, it gets\n",
        "    # x current state\n",
        "    # nx next state\n",
        "    # a selected actions\n",
        "    # r rewards obtained\n",
        "    def update(self,x,nx,a,r):\n",
        "        xi = self.observ2state(x)\n",
        "        nxi = self.observ2state(nx)\n",
        "\n",
        "        if self.agentType == 'central':\n",
        "            ai = np.ravel_multi_index(a,[self.NA,self.NA])\n",
        "            self.Q[xi,ai] += self.alpha * (np.sum(r) + self.gamma * np.max(self.Q[nxi,:]) - self.Q[xi,ai])\n",
        "        elif self.agentType == 'JALAM':\n",
        "            self.C[xi,a[self.id],a[1-self.id]] += 1\n",
        "            self.Q[xi,a[self.id],a[1-self.id]] += self.alpha * (r[self.id] + self.gamma * np.max(self.Q[nxi,:]) - self.Q[xi,a[self.id],a[1-self.id]])\n",
        "        else:\n",
        "            self.Q[xi,a[self.id]] += self.alpha * (r[self.id] + self.gamma * np.max(self.Q[nxi,:]) - self.Q[xi,a[self.id]])\n",
        "\n",
        "        return self.Q[x,:]\n",
        "\n",
        "    # choosing the action to make in a given state x\n",
        "    def chooseAction(self,x,e):\n",
        "        xi = self.observ2state(x)\n",
        "        if type(e) is float:\n",
        "            if self.agentType == 'central':\n",
        "                A = egreedy(self.Q[xi,:],e=e)\n",
        "                a = np.unravel_index(A,[self.NA,self.NA])\n",
        "                return a[self.id]\n",
        "            elif self.agentType == 'JALAM':\n",
        "                policyB = np.sum(self.C[xi,:,:],axis = 1)\n",
        "                policyB = policyB/np.sum(policyB)\n",
        "\n",
        "                bestreponseA = self.Q[xi,:,:] @ policyB.T\n",
        "\n",
        "                actA = np.argmax(bestreponseA)\n",
        "                return actA\n",
        "            else:\n",
        "                return egreedy(self.Q[xi,:],e=e)\n",
        "\n",
        "\n",
        "# tun the algorithm for N steps\n",
        "# x0 - if an initial state is provided the agents always start there, if not a random state is always generate after one prey escapes\n",
        "def run(N,x0=[],e = 0.9,gamma = 0.9, agentType = 'independent'):\n",
        "\n",
        "    if x0==[]:\n",
        "        x = [np.random.randint(1,5),np.random.randint(1,5),0,5]\n",
        "    else:\n",
        "        x = x0[:]\n",
        "    A = np.ones(2,dtype = int)\n",
        "    Log = []\n",
        "    Ag = [Agent(6,3,0,gamma = gamma,agentType = agentType), Agent(6,3,1,gamma = gamma,agentType = agentType)]\n",
        "\n",
        "    for ii in range(0,N):\n",
        "        A[0] = Ag[0].chooseAction(x, e)\n",
        "        A[1] = Ag[1].chooseAction(x, e)\n",
        "        nx,r = step(x,A)\n",
        "\n",
        "        Ag[0].update(x,nx,A,r)\n",
        "        Ag[1].update(x,nx,A,r)\n",
        "\n",
        "        #print(A-1,nx,r)\n",
        "        x = nx\n",
        "        if (x[2]==-1)|(x[3]==-1):\n",
        "            Log += [np.max(r)]\n",
        "            if x0==[]:\n",
        "                x = [np.random.randint(1,5),np.random.randint(1,5),0,5]\n",
        "            else:\n",
        "                x = x0[:]\n",
        "\n",
        "    return Ag, Log\n",
        "\n",
        "# this function allows to verify the action choosing in all possible initial states to verify what have the agents learned\n",
        "def test(Ag):\n",
        "\n",
        "    A = np.ones(2,dtype = int)\n",
        "    Log = np.zeros((NL,NL))\n",
        "    for i1 in [1,2,3,4]:\n",
        "        for i2 in [1,2,3,4]:\n",
        "            x0=[i1,i2,0,5]\n",
        "\n",
        "            x = x0[:]\n",
        "            for ii in range(0,100):\n",
        "                A[0] = Ag[0].chooseAction(x, e = 1.0)\n",
        "                A[1] = Ag[1].chooseAction(x, e = 1.0)\n",
        "                nx,r = step(x,A)\n",
        "\n",
        "                #print(A-1,nx,r)\n",
        "                x = nx\n",
        "                if (x[2]==-1)|(x[3]==-1):\n",
        "                    Log[i1,i2] = np.max(r)\n",
        "                    break\n",
        "\n",
        "    return Log[1:NL-1,1:NL-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compare the behaviour of different learning methods in the Stag-Hunt Markov Game $\\gamma=.95$\n",
        "for agentType in ['independent', 'observ', 'central','JALAM']:\n",
        "    print(agentType)\n",
        "    # run the different methods for 20000 steps\n",
        "    Ag, Log = run(20000, e = 0.9, gamma = 0.95, agentType = agentType)\n",
        "    Log = test(Ag)\n",
        "    print(\"Value if the agents start in the initial location given by the row-column in the matrix.\\n The second row, third column correspond to the initial state [3,4,0,5]\")\n",
        "    print(Log)"
      ],
      "metadata": {
        "id": "Qnu73mMSO7gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Compare the behaviour of different learning methods in the Stag-Hunt Markov Game $\\gamma=.75$\n",
        "for agentType in ['independent', 'observ', 'central','JALAM']:\n",
        "    print(agentType)\n",
        "    # run the different methods for 20000 steps\n",
        "    Ag, Log = run(20000, e = 0.9, gamma = 0.75, agentType = agentType)\n",
        "    # test the equilibria for all possible starting positions of the agents\n",
        "    Log = test(Ag)\n",
        "    print(\"Value if the agents start in the initial location given by the row-column in the matrix.\\n The second row, third column correspond to the initial state [3,4,0,5]\")\n",
        "    print(Log)"
      ],
      "metadata": {
        "id": "KBccelsehKGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "\n",
        "* What is the impact of $\\epsilon$ in the learning?\n",
        "* What does it mean to have a centralized learning in this context? Can we have two rewards? Discuss.\n",
        "* What is the impact of the initial location of the agents? In particular for the independent case.\n"
      ],
      "metadata": {
        "id": "CIpk-AI1iRGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Questions\n",
        "* Can you implement a version of minimaxQ (using the demo code provided)?\n",
        "* Change the game to allow it to continue even after getting the hare. What new behaviors will show?"
      ],
      "metadata": {
        "id": "vnhHqdbyi2wj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bwk5fRn1dwaq"
      },
      "outputs": [],
      "source": [
        "# @title Bonus Question \"MinimaxQ\"\n",
        "# If interested, you can replace the best response in Ficticious Play with the MinimaxQ approach, chapter 6.2 from the book.\n",
        "import numpy as np\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "def minimaxQ(Q):\n",
        "    n0, n1 = 2, 2\n",
        "    c = np.zeros(n0 + 1)\n",
        "    c[0] = -1\n",
        "    A_ub = np.ones((n1, n0 + 1))\n",
        "    A_ub[:, 1:] = -Q[:n0, :n1].T\n",
        "    b_ub = np.zeros(n1)\n",
        "    A_eq = np.ones((1, n0 + 1))\n",
        "    A_eq[0, 0] = 0\n",
        "    b_eq = [1]\n",
        "    bounds = ((None, None),) + ((0, 1),) * n0\n",
        "\n",
        "    res = linprog(c, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq, bounds=bounds)\n",
        "\n",
        "    return res\n",
        "\n",
        "# Q = np.array([[6,4],[2,5]])\n",
        "# res = minimaxQ(Q)\n",
        "# print(res.x[1:])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}